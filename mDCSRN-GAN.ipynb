{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2b92f2c89e4ed1b1f1fed7dc5d4087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='path')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "file_path = widgets.Text(\n",
    "    description = 'path'\n",
    ")\n",
    "\n",
    "\n",
    "display(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(file_path.value)\n",
    "\n",
    "num_unit = int(config['NN']['n_u'])\n",
    "num_block = int(config['NN']['n_block'])\n",
    "growth = int(config['NN']['growth'])\n",
    "\n",
    "lr = float(config['NN']['lr'])\n",
    "beta1 = float(config['NN']['beta1'])\n",
    "alpha = float(config['NN']['alpha'])\n",
    "\n",
    "im_size = int(config['Training']['im_size'])\n",
    "n_epoch = int(config['Training']['n_epoch'])\n",
    "BATCH_SIZE = int(config['Training']['BATCH_SIZE'])\n",
    "hr_path = config['Training']['hr_path']\n",
    "lr_path = config['Training']['lr_path']\n",
    "lr_path_test = config['Training']['lr_test_data']\n",
    "hr_path_test = config['Training']['hr_test_data']\n",
    "log_name = config['Training']['log']\n",
    "checkpoint_dir = config['Training']['checkpoint_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ctimage(Dataset):\n",
    "    def __init__(self, path_hr, path_lr):\n",
    "        self.img_hr = h5py.File(path_hr, 'r')\n",
    "        self.img_lr = h5py.File(path_lr, 'r')\n",
    "    def __del__(self):\n",
    "        self.img_hr.close()\n",
    "        self.img_lr.close()\n",
    "    def __len__(self):\n",
    "        return self.img_hr['data'].shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        #one sided label smoothing\n",
    "        return torch.from_numpy(np.expand_dims(self.img_hr['data'][idx],axis = 0)), torch.from_numpy(np.expand_dims(self.img_lr['data'][idx],axis = 0))\n",
    "\n",
    "training_data = ctimage(hr_path,lr_path)\n",
    "test_data = ctimage(hr_path_test,lr_path_test)\n",
    "\n",
    "dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (lrelu1): LeakyReLU(negative_slope=0.01)\n",
       "  (block1): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block2): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block3): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block4): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block5): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block6): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (block7): DiscriminatorBlock(\n",
       "    (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "    (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (ada): AdaptiveAvgPool3d(output_size=1)\n",
       "  (Dense1): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (lrelu2): LeakyReLU(negative_slope=0.01)\n",
       "  (Dense2): Conv3d(1024, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import DenseNet as dn\n",
    "from torch import nn\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv3d:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif type(m) == nn.BatchNorm3d:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)        \n",
    "\n",
    "netG = dn.Generator(4,4)#fill input\n",
    "netD = dn.Discriminator()\n",
    "\n",
    "netG.apply(init_weights)\n",
    "netD.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "gen_loss = dn.GeneratorLoss()\n",
    "\n",
    "if(torch.cuda.device_count()>1):\n",
    "    netG = nn.DataParallel(netG)\n",
    "    netD = nn.DataParallel(netD)\n",
    "    criterion = criterion.cuda()\n",
    "    gen_loss = gen_loss.cuda()\n",
    "    \n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "elif(torch.cuda.is_available()):\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    gen_loss = gen_loss.cuda()\n",
    "\n",
    "d_optimizer = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, 0.999))\n",
    "g_optimizer = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, 0.999))\n",
    "\n",
    "d_schedule = optim.lr_scheduler.StepLR(d_optimizer, step_size=500, gamma=0.1)\n",
    "g_schedule = optim.lr_scheduler.StepLR(g_optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:31<00:00, 20.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import tensor\n",
    "import torchvision\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(comment=log_name)\n",
    "\n",
    "gen_iterations = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        netD.train()\n",
    "        netG.train()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        hr_img = sample_batched[0].float().cuda()\n",
    "        lr_img = sample_batched[1].float().cuda()\n",
    "        \n",
    "        real_label = tensor([0.9]*BATCH_SIZE, dtype = torch.float).cuda()  \n",
    "        \n",
    "        #train with real\n",
    "        output = netD(hr_img)\n",
    "        errD_real = criterion(output, real_label)\n",
    "        errD_real.backward()\n",
    "        \n",
    "        fake = netG(lr_img)\n",
    "        \n",
    "        output = netD(fake.detach())\n",
    "        \n",
    "        fake_label = tensor([0.0]*BATCH_SIZE, dtype = torch.float).cuda()\n",
    "        errD_fake = criterion(output, fake_label)\n",
    "        errD_fake.backward()\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        d_optimizer.step()\n",
    "        \n",
    "        netG.zero_grad()\n",
    "        real_label = tensor([1.0]*BATCH_SIZE, dtype = torch.float).cuda() # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = gen_loss(output, real_label, fake, hr_img)\n",
    "        errG.backward()\n",
    "\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('Loss/d',errD.data.item(),gen_iterations)\n",
    "        #writer.add_scalar('Loss/d',0,gen_iterations)\n",
    "        writer.add_scalar('Loss/g',errG.data.item(),gen_iterations)\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "    work_dir = \"training_checkpoints\"\n",
    "    d_schedule.step()\n",
    "    g_schedule.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        if(torch.cuda.device_count()>1):\n",
    "            G_Data = netG.module.state_dict()\n",
    "            D_Data = netD.module.state_dict()\n",
    "        else:\n",
    "            G_Data = netG.state_dict()\n",
    "            D_Data = netD.state_dict()\n",
    "        torch.save(G_Data, os.path.join(\".\",work_dir,checkpoint_dir,\"netG_epoch_{}.pth\".format(epoch)))\n",
    "        torch.save(D_Data, os.path.join(\".\",work_dir,checkpoint_dir,\"netD_epoch_{}.pth\".format(epoch)))\n",
    "        \n",
    "        netD.eval()\n",
    "        netG.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i_batch, sample_batched in enumerate(test_dataloader):\n",
    "                hr_img_t = sample_batched[0].float().cuda()\n",
    "                lr_img_t = sample_batched[1].float().cuda()\n",
    "                \n",
    "                result_img = netG(lr_img_t)\n",
    "                output = netD(result_img)\n",
    "                errT = gen_loss(output,real_label,result_img,hr_img_t)\n",
    "                \n",
    "                writer.add_images('test output', result_img[:,:,:,:,10], epoch)\n",
    "            \n",
    "                writer.add_scalar('Loss/g test', errT.item(), epoch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
