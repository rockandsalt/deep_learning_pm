{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ae57f692ad4a7d9634dd3e8622aa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntText(value=512, description='nz'), IntText(value=32, description='ngf'), IntT…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# size of latent z vector\n",
    "nz = widgets.IntText(\n",
    "    value=512,\n",
    "    description='nz'\n",
    ")\n",
    "\n",
    "ngf =  widgets.IntText(\n",
    "    value=32,\n",
    "    description='ngf'\n",
    ")\n",
    "\n",
    "ndf =  widgets.IntText(\n",
    "    value=16,\n",
    "    description='ndf'\n",
    ")\n",
    "\n",
    "n_epoch = widgets.IntText(\n",
    "    value=25,\n",
    "    description='n epoch'\n",
    ")\n",
    "\n",
    "lr = widgets.FloatText(\n",
    "    value = 0.002,\n",
    "    description = 'learning rate'\n",
    ")\n",
    "\n",
    "beta1 = widgets.FloatText(\n",
    "    value = 0.5,\n",
    "    description = 'beta 1'\n",
    ")\n",
    "\n",
    "im_size = widgets.IntText(\n",
    "    value = 64,\n",
    "    description = 'im size'\n",
    ")\n",
    "\n",
    "file_path = widgets.Text(\n",
    "    description = 'path'\n",
    ")\n",
    "\n",
    "u_box = widgets.HBox([nz, ngf, ndf, n_epoch])\n",
    "b_box = widgets.HBox([lr, beta1,im_size])\n",
    "v_b = widgets.VBox([u_box,b_box,file_path])\n",
    "\n",
    "display(v_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "class ctimage(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.img = h5py.File(path, 'r')['data']\n",
    "    def __len__(self):\n",
    "        return self.img.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        #one sided label smoothing\n",
    "        return torch.from_numpy(np.expand_dims(self.img[idx],axis = 0)), 0.9\n",
    "\n",
    "training_data = ctimage(file_path.value)\n",
    "\n",
    "dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv3d(1, 16, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (3): Conv3d(16, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (6): Conv3d(32, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (9): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "  (10): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): LeakyReLU(negative_slope=0.2, inplace)\n",
       "  (12): Conv3d(128, 1, kernel_size=(4, 4, 4), stride=(1, 1, 1), bias=False)\n",
       "  (13): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dcgan\n",
    "from torch import nn\n",
    "nc = 1\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv3d:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif type(m) == nn.BatchNorm3d:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)        \n",
    "\n",
    "netG = dcgan.make_generator_model(im_size.value, nz.value, nc, ngf.value)\n",
    "netD = dcgan.make_discriminator_model(im_size.value, nc, ndf.value)\n",
    "\n",
    "netG.apply(init_weights)\n",
    "netD.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)\n",
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    netG = netG.cuda()\n",
    "    netD = netD.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "d_optimizer = optim.Adam(netD.parameters(), lr = lr.value, betas = (beta1.value, 0.999))\n",
    "g_optimizer = optim.Adam(netG.parameters(), lr = lr.value, betas = (beta1.value, 0.999))\n",
    "\n",
    "d_schedule = optim.lr_scheduler.StepLR(d_optimizer, step_size=500, gamma=0.1)\n",
    "g_schedule = optim.lr_scheduler.StepLR(g_optimizer, step_size=500, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 728/3000 [35:20:46<111:32:02, 176.73s/it]Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-df56fcdff8eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2113\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch import tensor\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "\n",
    "training_curve = \"training_curve\"\n",
    "\n",
    "num_iteration = len(dataloader)*n_epoch.value\n",
    "\n",
    "hf = h5py.File(os.path.join('.',training_curve,'training_curve.hdf5'), \"w\")\n",
    "loss_d = hf.create_dataset(\"Loss D\", (num_iteration,), dtype='f')\n",
    "loss_g = hf.create_dataset(\"Loss G\", (num_iteration,), dtype='f')\n",
    "d_x = hf.create_dataset(\"D(x)\", (num_iteration,), dtype='f')\n",
    "d_g_z = hf.create_dataset(\"D(G(z))\", (num_iteration,2), dtype='f')\n",
    "\n",
    "gen_iterations = 0\n",
    "for epoch in tqdm(range(n_epoch.value)):\n",
    "    start = time.time()\n",
    "    d_schedule.step()\n",
    "    g_schedule .step()\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_img = sample_batched[0].to(device, dtype=torch.float)\n",
    "        real_label = sample_batched[1].to(device, dtype=torch.float)        \n",
    "        \n",
    "        #train with real\n",
    "        output = netD(real_img)\n",
    "        errD_real = criterion(output, real_label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "        \n",
    "        noise = torch.rand((BATCH_SIZE,nz.value,1,1,1)).to(device, dtype=torch.float)\n",
    "        fake = netG(noise).detach()\n",
    "        \n",
    "        output = netD(fake)\n",
    "        \n",
    "        fake_label = tensor([0.0]*BATCH_SIZE, dtype = torch.float).to(device)\n",
    "        errD_fake = criterion(output, fake_label)\n",
    "        errD_fake.backward()\n",
    "        \n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        g_iter = 1\n",
    "        \n",
    "        while g_iter != 0:\n",
    "            netG.zero_grad()\n",
    "            real_label = tensor([1.0]*BATCH_SIZE, dtype = torch.float).to(device) # fake labels are real for generator cost\n",
    "            noise = torch.ones(())\n",
    "            noise = noise.new_empty((BATCH_SIZE,nz.value,1,1,1), dtype=torch.float, device=device)\n",
    "            noise.normal_(0,1)\n",
    "            fake = netG(noise)\n",
    "            output = netD(fake)\n",
    "            errG = criterion(output, real_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.data.mean()\n",
    "            g_optimizer.step()\n",
    "            g_iter -= 1\n",
    "        \n",
    "        loss_d[gen_iterations] = errD.data.item()\n",
    "        loss_g[gen_iterations] = errG.data.item()\n",
    "        d_x[gen_iterations] = D_x.cpu()\n",
    "        d_g_z[gen_iterations][0] = D_G_z1\n",
    "        d_g_z[gen_iterations][1] = D_G_z2\n",
    "        \n",
    "        gen_iterations += 1\n",
    "        \n",
    "    work_dir = \"training_checkpoints\"\n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(netG.state_dict(), os.path.join(\".\",work_dir,\"netG_epoch_{}.pth\".format(epoch)))\n",
    "        torch.save(netD.state_dict(), os.path.join(\".\",work_dir,\"netD_epoch_{}.pth\".format(epoch)))\n",
    "        hf.flush()\n",
    "\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
